{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import custom pipeline classes from _pipefunctions.py_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipefunctions import *\n",
    "from auxiliar_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--input_data\", metavar=\"FILE\",\n",
    "        help = \"Input data for the experiment.\")\n",
    "parser.add_argument(\"-o\", \"--output_folder\", metavar=\"FILE\",\n",
    "        help = \"Output folder path for experiment results.\")\n",
    "parser.add_argument(\"-f\", \"--jupyter_config\", metavar=\"FILE\",\n",
    "        help = \"STUB. This is here to ignore jupyter's config.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.jupyter_config is not None: \n",
    "    print(\"Executing on Jupyter. Setting default params...\")\n",
    "    input_data=\"crbmdata.csv\"\n",
    "    output_folder=\"/tmp/MainEngine_regression_loop-Jupyter\"\n",
    "else: \n",
    "    input_data = args.input_data\n",
    "    output_folder = args.output_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file=output_folder+\"/result.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "df = pd.read_csv(input_data)\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the pipeline we want to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Pipeline([\n",
    "        (\"dropNA\", Droper()), \n",
    "        ('logaritmizer', Logaritmizer(inputColumn='installedPowerME', outputColumn='logInstalledPowerME')),\n",
    "        ('stringCast', StringCaster(column='type')),\n",
    "        ('dummizer', Dummizer(inputColumns=['type'], outputPrefix='binType'))\n",
    "        # ('binner', Binner(inputColumn='logInstalledPowerME', outputColumn='binLogInstalledPowerME', bins=10)),\n",
    "        # ('binnerMid', BinnerMid(inputColumn='binLogInstalledPowerME', outputColumn='binmidLogInstalledPowerME')),\n",
    "        # ('stringCastBin', StringCaster(column='binmidLogInstalledPowerME'))\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we execute it with transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = preprocess.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imolist = df['imo'].drop_duplicates()\n",
    "nships = len(imolist)\n",
    "print(nships)\n",
    "# Sample IMOs\n",
    "np.random.seed(2)\n",
    "trainimo = np.random.choice(imolist, int(nships*0.8), replace=False)\n",
    "\n",
    "# Get data by IMO\n",
    "df_train =  df.loc[df['imo'].isin(trainimo)]\n",
    "df_test =  df.loc[~df['imo'].isin(trainimo)]\n",
    "\n",
    "# Train/Test groups\n",
    "group_train = df_train['imo']\n",
    "group_test = df_test['imo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'logInstalledPowerME'\n",
    "ptn = PandasToNumpyXY(response=target)\n",
    "\n",
    "k = 3\n",
    "cv = GroupKFold(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "binType = [col for col in df.columns if 'binType_' in col]\n",
    "f_act = [col for col in df.columns if 'activations' in col]\n",
    "f_rot = [col for col in df.columns if ('rotationGPS' in col and \n",
    "                                       'rotationGPSA' not in col and\n",
    "                                       'rotationGPSW' not in col)]\n",
    "f_sog = [col for col in df.columns if 'sog' in col]\n",
    "f_bat = [col for col in df.columns if 'bathymetry' in col]\n",
    "\n",
    "# Datasets\n",
    "# feat_type = binType \n",
    "# feat_all =  f_act + f_rot + f_sog + f_bat + binType\n",
    "feat_activation =  f_act + binType\n",
    "feat_history = f_rot + f_sog + f_bat + binType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Average params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = Meanizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid = {'alpha': [0.0001,0.001,0.01,0.1]}\n",
    "lasso = sklearn.linear_model.Lasso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_grid = {\n",
    "    'max_depth': [3,5],\n",
    "    'min_samples_split': [2,5],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.0001,0.001, 0.01, 0.1]\n",
    "}\n",
    "gb = sklearn.ensemble.GradientBoostingRegressor(n_estimators=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = {\n",
    "    'max_features': [\"auto\", \"sqrt\", \"log2\"],\n",
    "    'n_estimators': [200, 1000],\n",
    "    'max_depth': [5,10, None]\n",
    "}\n",
    "\n",
    "rf = sklearn.ensemble.RandomForestRegressor(n_estimators=200, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the SVM were not got enough for the execution time that it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_grid = None\n",
    "\n",
    "# svm = sklearn.svm.SVR(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "##  Final Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['Global average'] = dict({\n",
    "                        'target':target,\n",
    "                        'grid':None,\n",
    "                        'features':None,\n",
    "                        'model':avg\n",
    "                 })\n",
    "\n",
    "params['Type average'] = dict({\n",
    "                        'target':target,\n",
    "                        'grid':None,\n",
    "                        'features':feat_type,\n",
    "                        'model':avg\n",
    "                 })\n",
    "\n",
    "params['Lasso Activations'] = dict({\n",
    "                        'target':target,\n",
    "                        'grid':lasso_grid,\n",
    "                        'features':feat_activation,\n",
    "                        'model':lasso\n",
    "                 })\n",
    "\n",
    "params['Lasso History'] = dict({\n",
    "                        'target':target,\n",
    "                        'grid':lasso_grid,\n",
    "                        'features':feat_history,\n",
    "                        'model':lasso\n",
    "                 })\n",
    "\n",
    "params['GB Activations'] = dict({\n",
    "                        'target':target,\n",
    "                        'grid':gb_grid,\n",
    "                        'features':feat_activation,\n",
    "                        'model':gb\n",
    "                 })\n",
    "\n",
    "params['GB History'] = dict({\n",
    "                        'target':target,\n",
    "                        'grid':gb_grid,\n",
    "                        'features':feat_history,\n",
    "                        'model':gb\n",
    "                 })\n",
    "\n",
    "params['RF Activations'] = dict({\n",
    "                        'target':target,\n",
    "                        'grid':rf_grid,\n",
    "                        'features':feat_activation,\n",
    "                        'model':rf\n",
    "                 })\n",
    "\n",
    "params['RF History'] = dict({\n",
    "                        'target':target,\n",
    "                        'grid':rf_grid,\n",
    "                        'features':feat_history,\n",
    "                        'model':rf\n",
    "                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params['SVM Activations'] = dict({\n",
    "#                         'target':target,\n",
    "#                         'grid':svm_grid,\n",
    "#                         'features':feat_activation,\n",
    "#                         'model':svm\n",
    "#                  })\n",
    "# \n",
    "# params['SVM History'] = dict({\n",
    "#                         'target':target,\n",
    "#                         'grid':rf_grid,\n",
    "#                         'features':feat_history,\n",
    "#                         'model':svm\n",
    "#                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model fitting loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for ke in params:\n",
    "    p = params[ke]\n",
    "    modelname = ke\n",
    "    target = p['target']\n",
    "    grid = p['grid']\n",
    "    if p['features'] is not None:\n",
    "        featuresResponse = p['features'] + [target]\n",
    "    else:\n",
    "        featuresResponse = [target]\n",
    "    model = sklearn.base.clone(p['model']) # Create a new model from base\n",
    "    \n",
    "\n",
    "    X_tr, y_tr = ptn.transform(df_train[featuresResponse])\n",
    "    X_te, y_te = ptn.transform(df_test[featuresResponse])\n",
    "\n",
    "    n_jobs = -1\n",
    "\n",
    "    if (grid is not None):\n",
    "        gr =  GridSearchCV( model, param_grid=grid, n_jobs = n_jobs, cv=cv)\n",
    "        gr.fit(X_tr, y_tr, groups=group_train)\n",
    "        mean_time = np.mean(gr.cv_results_['mean_fit_time'])\n",
    "        start_time = time.time()\n",
    "        e = predict_results(gr, X_tr, y_tr, X_te, y_te, group_train, group_test, modelname)\n",
    "        predict_time = time.time() - start_time\n",
    "        model = gr\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        mean_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        e = predict_results(model, X_tr, y_tr, X_te, y_te, group_train, group_test, modelname)\n",
    "        predict_time = time.time() - start_time\n",
    "        \n",
    "    \n",
    "    \n",
    "    print(\n",
    "        \"\"\"{}: \n",
    "        - Mean fit time: {}\n",
    "        - Predict time: {}\n",
    "        - Train Median MAE: {}\n",
    "        - Test Median MAE: {}\n",
    "        - Train Mean MAE: {}\n",
    "        - Test Mean MAE: {}\n",
    "        \"\"\".format(ke, mean_time, predict_time,\n",
    "                   e['TrainMedianMAE'],e['TestMedianMAE'],\n",
    "                   e['TrainMeanMAE'],e['TestMeanMAE']))\n",
    "    \n",
    "    pres = pd.DataFrame({**e, 'mean_time':mean_time, 'predict_time':predict_time})\n",
    "    res = res.append(pres)\n",
    "    \n",
    "    # Save Model\n",
    "    joblib.dump(model, output_folder+'/'+modelname.replace(\" \", \"_\")+\".sav\")\n",
    "    \n",
    "    # Save metrics - If file doesn't exist, put header.\n",
    "    pres.to_csv(results_file, index=False, mode=\"a\", \n",
    "           header=(not os.path.isfile(results_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare best models results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = joblib.load('models/RF_History.sav')\n",
    "X_tr, y_tr = ptn.transform(df_train[feat_history+[target]])\n",
    "X_te, y_te = ptn.transform(df_test[feat_history+[target]])\n",
    "pred_tr = m.predict(X_tr)\n",
    "pred_te = m.predict(X_te)\n",
    "getErrorMeasures(np.exp(y_te), np.exp(pred_te), group=df_test['imo'], agg_funct='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = joblib.load('models/RF_Activations.sav')\n",
    "X_tr, y_tr = ptn.transform(df_train[feat_activation+[target]])\n",
    "X_te, y_te = ptn.transform(df_test[feat_activation+[target]])\n",
    "pred_tr = m.predict(X_tr)\n",
    "pred_te = m.predict(X_te)\n",
    "getErrorMeasures(np.exp(y_te), np.exp(pred_te), group=df_test['imo'], agg_funct='median')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
